{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予備役あるいは非戦闘任務から処分イベントへの移行に関するEHAデータの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1512,
     "status": "ok",
     "timestamp": 1762884194904,
     "user": {
      "displayName": "naoki cocaze",
      "userId": "16247746366338060822"
     },
     "user_tz": -540
    },
    "id": "jGYXNPqxqeEf",
    "outputId": "3df4f894-cd9a-4c07-c293-353e1af22cdb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# --- 設定 ---\n",
    "BASE_DIR = 'https://github.com/naoki-kokaze/British_Warship_Career/tree/main/'\n",
    "# GraphDBで生成した生のログファイルを参照\n",
    "INPUT_CSV_PATH = os.path.join(BASE_DIR, 'data', 'presence_log.csv')\n",
    "# ステップC2の出力ファイル (2種類)\n",
    "OUTPUT_CSV_ORD = os.path.join(BASE_DIR, 'data', 'eha_dataset_disposal_from_Ordinary.csv')\n",
    "OUTPUT_CSV_NCD = os.path.join(BASE_DIR, 'data', 'eha_dataset_disposal_from_NonCombat.csv')\n",
    "LOG_FILE = os.path.join(BASE_DIR, 'log', 'eha_preparation_C.log')\n",
    "\n",
    "logging.basicConfig(filename=LOG_FILE, level=logging.INFO, filemode='w',\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "print(f\"Log will be saved to: {LOG_FILE}\")\n",
    "print(f\"Output CSV 1 (Ordinary) will be saved to: {OUTPUT_CSV_ORD}\")\n",
    "print(f\"Output CSV 2 (NonCombat) will be saved to: {OUTPUT_CSV_NCD}\")\n",
    "\n",
    "# --- 時代区分の定義 (仮説C用) ---\n",
    "PERIOD_PRE_WARRIOR = (1850, 1859)\n",
    "PERIOD_WARRIOR_ERA = (1860, 1869) # 仮説(i) 延命期\n",
    "PERIOD_PURGE_ERA = (1870, 1879)   # 仮説(ii) 整理期\n",
    "\n",
    "# --- ヘルパー関数 ---\n",
    "def parse_eha_date(date_str):\n",
    "    if pd.isna(date_str):\n",
    "        return pd.NaT\n",
    "    date_str = str(date_str).strip()\n",
    "    try:\n",
    "        if re.match(r'^\\d{4}$', date_str): # YYYY\n",
    "            return pd.to_datetime(f\"{date_str}-01-01\")\n",
    "        if re.match(r'^\\d{4}-\\d{2}$', date_str): # YYYY-MM\n",
    "            return pd.to_datetime(f\"{date_str}-01\")\n",
    "        if re.match(r'^\\d{4}-\\d{2}-\\d{2}', date_str): # YYYY-MM-DD...\n",
    "            return pd.to_datetime(date_str.split('T')[0])\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Could not parse date: {date_str}. Error: {e}\")\n",
    "        return pd.NaT\n",
    "    return pd.NaT\n",
    "\n",
    "def calculate_duration_years(start_date, end_date):\n",
    "    if pd.isna(start_date) or pd.isna(end_date):\n",
    "        return np.nan\n",
    "    try:\n",
    "        duration_days = (end_date - start_date).days\n",
    "        if duration_days <= 0:\n",
    "             return np.nan\n",
    "        return duration_days / 365.25\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def assign_period_flag_C(start_year):\n",
    "    \"\"\"仮説C用の時代区分フラグを割り当てる\"\"\"\n",
    "    if pd.isna(start_year):\n",
    "        return \"Other_Pre_1850\" # ベースライン\n",
    "    start_year = int(start_year)\n",
    "    if PERIOD_PRE_WARRIOR[0] <= start_year <= PERIOD_PRE_WARRIOR[1]:\n",
    "        return \"PreWarrior_1850s\"\n",
    "    if PERIOD_WARRIOR_ERA[0] <= start_year <= PERIOD_WARRIOR_ERA[1]:\n",
    "        return \"WarriorEra_1860s\"\n",
    "    if PERIOD_PURGE_ERA[0] <= start_year <= PERIOD_PURGE_ERA[1]:\n",
    "        return \"PurgeEra_1870s\"\n",
    "    if start_year < 1850:\n",
    "        return \"Other_Pre_1850\" # ベースライン\n",
    "    return \"Other_Post_1880\"\n",
    "\n",
    "def assign_category_group_C(category_code):\n",
    "    \"\"\"艦種グループを割り当てる (Bと同様)\"\"\"\n",
    "    if pd.isna(category_code):\n",
    "        return \"Unknown\"\n",
    "    code = str(category_code).split('_')[0]\n",
    "    if code.startswith('SC') or code.startswith('A') or code.startswith('P'):\n",
    "         return \"NewTech_SteamOrArmoured\" # ベースライン\n",
    "    if code in ['SL1', 'SL2', 'SL3', 'SL4']:\n",
    "        return \"Sailing_LineOfBattle\"\n",
    "    if code in ['SF5', 'SC6']:\n",
    "        return \"Sailing_FrigateCorvette\"\n",
    "    if code in ['SSL', 'SBS', 'SCS', 'SM']:\n",
    "        return \"Sailing_Other\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "# --- 共通の整形処理を行う関数 ---\n",
    "def process_eha_subset(df_subset, analysis_status_name):\n",
    "    \"\"\"\n",
    "    フィルタリングされたDataFrame (InOrdinary または InNonCombatDuty) を受け取り、\n",
    "    EHA変数（duration, event, age, period, category）を作成して返す\n",
    "    \"\"\"\n",
    "    print(f\"Processing {len(df_subset)} records for status: {analysis_status_name}\")\n",
    "    logging.info(f\"Processing {len(df_subset)} records for status: {analysis_status_name}\")\n",
    "\n",
    "    # 日付データのパース\n",
    "    df_subset['birth_date_dt'] = df_subset['birth_date'].apply(parse_eha_date)\n",
    "    df_subset['start_date_dt'] = df_subset['start_date'].apply(parse_eha_date)\n",
    "    df_subset['end_date_dt'] = df_subset['end_date'].apply(parse_eha_date)\n",
    "\n",
    "    # 必要な日付データが欠損している行を除外\n",
    "    df_cleaned = df_subset.dropna(subset=['birth_date_dt', 'start_date_dt', 'end_date_dt']).copy()\n",
    "    dropped_count = len(df_subset) - len(df_cleaned)\n",
    "    if dropped_count > 0:\n",
    "        print(f\"Dropped {dropped_count} rows due to missing essential date data.\")\n",
    "        logging.warning(f\"Dropped {dropped_count} rows due to missing essential date data.\")\n",
    "\n",
    "    if df_cleaned.empty:\n",
    "        print(f\"No valid records remaining for {analysis_status_name}.\")\n",
    "        logging.error(f\"No valid records remaining for {analysis_status_name}.\")\n",
    "        return None\n",
    "\n",
    "    # EHA変数の作成\n",
    "    print(f\"Creating EHA variables for {analysis_status_name}...\")\n",
    "\n",
    "    # duration: この状態だった期間（年）\n",
    "    df_cleaned['duration'] = df_cleaned.apply(\n",
    "        lambda row: calculate_duration_years(row['start_date_dt'], row['end_date_dt']),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # event: 1 = キャリア終了 (解体 E6_Destruction または 売却 E8_Acquisition)\n",
    "    disposal_events = ['E6_Destruction', 'E8_Acquisition']\n",
    "    df_cleaned['event'] = df_cleaned['end_event_type'].apply(\n",
    "        lambda x: 1 if x in disposal_events else 0\n",
    "    )\n",
    "\n",
    "    # age_at_start: この状態が始まった時点の艦齢（年）\n",
    "    df_cleaned['age_at_start'] = df_cleaned.apply(\n",
    "        lambda row: calculate_duration_years(row['birth_date_dt'], row['start_date_dt']),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # period_flag: 時代区分\n",
    "    df_cleaned['start_year'] = df_cleaned['start_date_dt'].dt.year\n",
    "    df_cleaned['period_flag'] = df_cleaned['start_year'].apply(assign_period_flag_C)\n",
    "\n",
    "    # category_group: 艦種グループ\n",
    "    df_cleaned['category_group'] = df_cleaned['category_code'].apply(assign_category_group_C)\n",
    "\n",
    "    # 最終クリーニング\n",
    "    df_final = df_cleaned.dropna(subset=['duration', 'age_at_start', 'category_group', 'period_flag'])\n",
    "    df_final = df_final[df_final['duration'] > 0]\n",
    "    df_final = df_final[df_final['category_group'] != 'Unknown']\n",
    "\n",
    "    print(f\"Final dataset for {analysis_status_name} contains {len(df_final)} observations.\")\n",
    "    logging.info(f\"Final dataset for {analysis_status_name} contains {len(df_final)} observations.\")\n",
    "\n",
    "    return df_final\n",
    "\n",
    "# --- メイン処理 ---\n",
    "def prepare_data_C():\n",
    "    print(\"--- Starting EHA Data Preparation (Hypothesis C: Disposal) ---\")\n",
    "    logging.info(\"--- Starting EHA Data Preparation (Hypothesis C: Disposal) ---\")\n",
    "\n",
    "    # 1. CSVファイルを読み込む\n",
    "    try:\n",
    "        print(f\"Loading presence log from: {INPUT_CSV_PATH}\")\n",
    "        df = pd.read_csv(INPUT_CSV_PATH)\n",
    "        logging.info(f\"Loaded {len(df)} records from {INPUT_CSV_PATH}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERROR: File not found. {e}\")\n",
    "        logging.error(f\"File not found: {e}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR reading CSV files: {e}\")\n",
    "        logging.error(f\"Error reading CSV files: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. 2つの分析対象をフィルタリング\n",
    "    df_ordinary = df[df['status'] == 'InOrdinary'].copy()\n",
    "    df_noncombat = df[df['status'] == 'InNonCombatDuty'].copy()\n",
    "\n",
    "    if df_ordinary.empty:\n",
    "        print(\"Warning: No records found with status 'InOrdinary'.\")\n",
    "        logging.warning(\"No records found with status 'InOrdinary'.\")\n",
    "    if df_noncombat.empty:\n",
    "        print(\"Warning: No records found with status 'InNonCombatDuty'.\")\n",
    "        logging.warning(\"No records found with status 'InNonCombatDuty'.\")\n",
    "\n",
    "    # 3. それぞれのデータセットを処理\n",
    "    df_final_ordinary = process_eha_subset(df_ordinary, 'InOrdinary')\n",
    "    df_final_noncombat = process_eha_subset(df_noncombat, 'InNonCombatDuty')\n",
    "\n",
    "    # 4. 最終的なCSVに保存\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(OUTPUT_CSV_ORD), exist_ok=True)\n",
    "        if df_final_ordinary is not None and not df_final_ordinary.empty:\n",
    "            df_final_ordinary.to_csv(OUTPUT_CSV_ORD, index=False, encoding='utf-8')\n",
    "            print(f\"Successfully saved EHA dataset for 'InOrdinary' to {OUTPUT_CSV_ORD}\")\n",
    "            logging.info(f\"Successfully saved EHA dataset for 'InOrdinary' to {OUTPUT_CSV_ORD}\")\n",
    "        else:\n",
    "            print(\"No data to save for 'InOrdinary'.\")\n",
    "            logging.warning(\"No data to save for 'InOrdinary'.\")\n",
    "\n",
    "        if df_final_noncombat is not None and not df_final_noncombat.empty:\n",
    "            df_final_noncombat.to_csv(OUTPUT_CSV_NCD, index=False, encoding='utf-8')\n",
    "            print(f\"Successfully saved EHA dataset for 'InNonCombatDuty' to {OUTPUT_CSV_NCD}\")\n",
    "            logging.info(f\"Successfully saved EHA dataset for 'InNonCombatDuty' to {OUTPUT_CSV_NCD}\")\n",
    "        else:\n",
    "            print(\"No data to save for 'InNonCombatDuty'.\")\n",
    "            logging.warning(\"No data to save for 'InNonCombatDuty'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        msg = f\"Error saving CSV files: {e}\"\n",
    "        print(msg)\n",
    "        logging.error(msg)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # データ整形処理を実行\n",
    "    prepare_data_C()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from lifelines import KaplanMeierFitter, CoxPHFitter\n",
    "import logging\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# --- 設定 ---\n",
    "BASE_DIR = 'https://github.com/naoki-kokaze/British_Warship_Career/tree/main/'\n",
    "# GraphDBで生成した生のログファイルを参照\n",
    "INPUT_DIR  = os.path.join(BASE_DIR, 'data')\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, 'visualizations')\n",
    "# ステップC2で生成したCSVファイル (2種類)\n",
    "INPUT_CSV_ORD = os.path.join(INPUT_DIR, 'eha_dataset_disposal_from_Ordinary.csv')\n",
    "INPUT_CSV_NCD = os.path.join(INPUT_DIR, 'eha_dataset_disposal_from_NonCombat.csv')\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ログ\n",
    "logging.basicConfig(filename=LOG_FILE, level=logging.INFO, filemode='w',\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "print(f\"Log will be saved to: {LOG_FILE}\")\n",
    "print(f\"Visualization outputs will be saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "# スタイル\n",
    "sns.set(style=\"whitegrid\")\n",
    "print(\"Seaborn style set.\")\n",
    "\n",
    "\n",
    "def stars(p):\n",
    "    return \"***\" if p < 0.001 else \"**\" if p < 0.01 else \"*\" if p < 0.05 else \"\"\n",
    "\n",
    "# --- メイン処理（1ファイル分） ---\n",
    "def run_eha_analysis_C(input_csv_path, analysis_name_suffix):\n",
    "    \"\"\"\n",
    "    指定されたCSVパスのデータを読み込み、KM と CoxPH（HR図にp値＋2段階帯）を出力\n",
    "    analysis_name_suffix: \"from_Ordinary\" / \"from_NonCombatDuty\" 等\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting EHA Analysis for: {analysis_name_suffix} ---\")\n",
    "    logging.info(f\"--- Starting EHA Analysis for: {analysis_name_suffix} ---\")\n",
    "\n",
    "    # 1) 読み込み\n",
    "    try:\n",
    "        print(f\"Loading EHA dataset from: {input_csv_path}\")\n",
    "        df = pd.read_csv(input_csv_path)\n",
    "        logging.info(f\"Loaded {len(df)} records from {input_csv_path}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERROR: File not found. {e}\")\n",
    "        logging.error(f\"File not found: {e}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR reading CSV files: {e}\")\n",
    "        logging.error(f\"Error reading CSV files: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2) 必須列チェック\n",
    "    required_cols = ['duration', 'event', 'age_at_start', 'period_flag', 'category_group']\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"ERROR: CSV is missing required columns for EHA: {', '.join(missing)}\")\n",
    "        logging.error(f\"CSV is missing required columns for EHA: {', '.join(missing)}\")\n",
    "        return\n",
    "\n",
    "    # 3) クリーニング\n",
    "    df_eha = df.dropna(subset=['duration', 'age_at_start'])\n",
    "    df_eha = df_eha[np.isfinite(df_eha['duration']) & np.isfinite(df_eha['age_at_start'])]\n",
    "\n",
    "    if df_eha.empty:\n",
    "        print(f\"No valid data for EHA ({analysis_name_suffix}) after cleaning NaN/Inf.\")\n",
    "        logging.error(f\"No valid data for EHA ({analysis_name_suffix}) after cleaning NaN/Inf.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Using {len(df_eha)} observations for EHA ({analysis_name_suffix}).\")\n",
    "\n",
    "    # 4) KM\n",
    "    \"\"\"\n",
    "    print(f\"Generating Kaplan-Meier Plot (Disposal by Period) for {analysis_name_suffix}...\")\n",
    "    logging.info(f\"Generating Kaplan-Meier Plot (Disposal by Period) for {analysis_name_suffix}\")\n",
    "\n",
    "    kmf = KaplanMeierFitter()\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ax_km = plt.subplot(111)\n",
    "\n",
    "    periods_to_plot = ['PreWarrior_1850s', 'WarriorEra_1860s', 'PurgeEra_1870s', 'Other_Post_1880', 'Other_Pre_1850']\n",
    "    for period in periods_to_plot:\n",
    "        data = df_eha[df_eha['period_flag'] == period]\n",
    "        if not data.empty:\n",
    "            kmf.fit(data['duration'], data['event'], label=f\"{period} (N={len(data)})\")\n",
    "            kmf.plot_survival_function(ax=ax_km)\n",
    "\n",
    "    plt.title(f'Kaplan-Meier: Survival from Disposal (from {analysis_name_suffix})', fontsize=16)\n",
    "    plt.xlabel(f'Duration in \"{analysis_name_suffix}\" (Years)', fontsize=12)\n",
    "    plt.ylabel('Probability of *Not* Being Disposed (Sold/Broken Up)', fontsize=12)\n",
    "    plt.legend(title='Period Started This Status')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    km_plot_path = os.path.join(OUTPUT_DIR, f'km_plot_disposal_{analysis_name_suffix}.png')\n",
    "    try:\n",
    "        plt.savefig(km_plot_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Kaplan-Meier plot (Disposal) saved to {km_plot_path}\")\n",
    "        logging.info(f\"Kaplan-Meier plot (Disposal) saved to {km_plot_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving KM plot (Disposal): {e}\")\n",
    "        logging.error(f\"Error saving KM plot (Disposal): {e}\")\n",
    "    plt.close()\n",
    "    \"\"\"\n",
    "\n",
    "    # 5) CoxPH\n",
    "    print(f\"Running Cox Proportional Hazards Model (Disposal) for {analysis_name_suffix}...\")\n",
    "    logging.info(f\"Running Cox Proportional Hazards Model (Disposal) for {analysis_name_suffix}\")\n",
    "\n",
    "    try:\n",
    "        df_cox = pd.get_dummies(\n",
    "            df_eha,\n",
    "            columns=['period_flag', 'category_group'],\n",
    "            prefix={'period_flag': 'period', 'category_group': 'cat'},\n",
    "            drop_first=True, dtype=int\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR creating dummy variables: {e}\")\n",
    "        logging.error(f\"ERROR creating dummy variables: {e}\")\n",
    "        return\n",
    "\n",
    "    period_dummies   = [c for c in df_cox.columns if c.startswith('period_')]\n",
    "    category_dummies = [c for c in df_cox.columns if c.startswith('cat_')]\n",
    "    covariates = ['age_at_start'] + period_dummies + category_dummies\n",
    "\n",
    "    final_cols = ['duration', 'event'] + covariates\n",
    "    if any(c not in df_cox.columns for c in final_cols):\n",
    "        missing_in_df = [c for c in final_cols if c not in df_cox.columns]\n",
    "        print(f\"ERROR: Columns missing after dummy creation: {missing_in_df}\")\n",
    "        logging.error(f\"Columns missing after dummy creation: {missing_in_df}\")\n",
    "        return\n",
    "\n",
    "    df_cox_final = df_cox[final_cols].copy()\n",
    "    df_cox_final = df_cox_final.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "    if df_cox_final.empty or not covariates:\n",
    "        print(f\"ERROR: No data or no covariates left for CoxPH model. Covariates found: {len(covariates)}\")\n",
    "        logging.error(f\"ERROR: No data or no covariates left for CoxPH model. Covariates: {len(covariates)}\")\n",
    "        return\n",
    "\n",
    "    cph = CoxPHFitter(penalizer=1e-5, l1_ratio=0.0)  # 安定化の弱いRidge\n",
    "    try:\n",
    "        cph.fit(df_cox_final, duration_col='duration', event_col='event')\n",
    "\n",
    "        # サマリ保存\n",
    "        summary = cph.summary\n",
    "        print(f\"\\n--- CoxPH Model Summary (Disposal from {analysis_name_suffix}) ---\")\n",
    "        print(summary)\n",
    "        logging.info(f\"\\n--- CoxPH Model Summary (Disposal from {analysis_name_suffix}) ---\\n{summary.to_string()}\")\n",
    "\n",
    "        summary_path = os.path.join(OUTPUT_DIR, f'coxph_summary_disposal_{analysis_name_suffix}.csv')\n",
    "        summary.to_csv(summary_path, encoding='utf-8-sig')\n",
    "        print(f\"Model summary (Disposal) saved to {summary_path}\")\n",
    "\n",
    "        # --- HR図（p値整列＋2段階帯） ---\n",
    "        plt.figure(figsize=(10, max(12, len(covariates) * 0.5)))\n",
    "        ax = cph.plot(hazard_ratios=True)\n",
    "        ax.set_xscale('log')\n",
    "        ax.axvline(1, ls='--', color='gray', alpha=.8)\n",
    "        ax.set_xticks([0.25, 0.5, 1, 2, 4])\n",
    "        ax.get_xaxis().set_major_formatter(mticker.ScalarFormatter())\n",
    "\n",
    "        # 帯の設定\n",
    "        sig_color    = '#FFF3B0'  # p<0.05（濃い黄）\n",
    "        sig_alpha    = 0.55\n",
    "        marg_color   = '#FFFBE6'  # 0.05<=p<0.07（薄い黄）\n",
    "        marg_alpha   = 0.9\n",
    "        p_sig        = 0.05\n",
    "        p_marg_upper = 0.07\n",
    "\n",
    "        # yラベル→座標\n",
    "        yticks  = ax.get_yticks()\n",
    "        ylabels = [t.get_text() for t in ax.get_yticklabels()]\n",
    "        label2y = {lab: y for lab, y in zip(ylabels, yticks)}\n",
    "\n",
    "        # 右外にp値を並べる\n",
    "        x_min, x_max = ax.get_xlim()\n",
    "        x_text = x_max * 1.05\n",
    "\n",
    "        for var, row in summary.iterrows():\n",
    "            if var not in label2y:\n",
    "                continue\n",
    "            y = label2y[var]\n",
    "            pval = float(row.get('p', 1.0))\n",
    "\n",
    "            # 2段階ハイライト\n",
    "            if pval < p_sig:\n",
    "                ax.axhspan(y - 0.35, y + 0.35, color=sig_color,  alpha=sig_alpha,  zorder=0)\n",
    "            elif p_sig <= pval < p_marg_upper:\n",
    "                ax.axhspan(y - 0.35, y + 0.35, color=marg_color, alpha=marg_alpha, zorder=0)\n",
    "\n",
    "            # p値注記（右外）\n",
    "            ax.text(x_text, y, f\"p = {pval:.3f}{stars(pval)}\", va='center', fontsize=10)\n",
    "\n",
    "        # 注記が切れないように右へ少し拡張\n",
    "        ax.set_xlim(x_min, x_text * 1.12)\n",
    "\n",
    "        # 帯の凡例を追加\n",
    "        patch_sig  = mpatches.Patch(facecolor=sig_color,  alpha=sig_alpha,  label=f'p < {p_sig}')\n",
    "        patch_marg = mpatches.Patch(facecolor=marg_color, alpha=marg_alpha, label=f'{p_sig} ≤ p < {p_marg_upper}')\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        ax.legend(handles + [patch_sig, patch_marg],\n",
    "                  labels + [patch_sig.get_label(), patch_marg.get_label()],\n",
    "                  loc='upper right', title='Significance')\n",
    "\n",
    "        plt.title(f'CoxPH Model: Hazard Ratios (exp(coef)) with p-values\\n(from {analysis_name_suffix})', fontsize=14)\n",
    "        plt.xlabel('Hazard Ratio (exp(coef))', fontsize=12)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "        out_svg = os.path.join(OUTPUT_DIR, f'coxph_plot_disposal_{analysis_name_suffix}_with_p_highlighted.svg')\n",
    "        plt.savefig(out_svg, format='svg', bbox_inches='tight')\n",
    "\n",
    "        print(f\"Model coefficients plot (with p-values) saved to {out_svg}\")\n",
    "        plt.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: CoxPH model fitting failed: {e}\")\n",
    "        logging.exception(f\"CoxPH model fitting failed: {e}\")\n",
    "\n",
    "    print(f\"--- EHA Analysis for {analysis_name_suffix} Finished ---\")\n",
    "    logging.info(f\"--- EHA Analysis for {analysis_name_suffix} Finished ---\")\n",
    "\n",
    "# --- エントリポイント ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 2つの分析を順番に実行\n",
    "    run_eha_analysis_C(INPUT_CSV_ORD, \"from_Ordinary\")\n",
    "    run_eha_analysis_C(INPUT_CSV_NCD, \"from_NonCombatDuty\")\n",
    "\n",
    "    print(\"\\n--- All EHA-C Analyses Finished ---\")\n",
    "    logging.info(\"--- All EHA-C Analyses Finished ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eha_dataset_retirement.csv 作成用のコードセル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14202,
     "status": "ok",
     "timestamp": 1762906612927,
     "user": {
      "displayName": "naoki cocaze",
      "userId": "16247746366338060822"
     },
     "user_tz": -540
    },
    "id": "qQVDS8NFa0G7",
    "outputId": "e5018824-bb0f-4cfd-91ed-a70af2df3043"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# --- 設定 ---\n",
    "BASE_DIR = 'https://github.com/naoki-kokaze/British_Warship_Career/tree/main/'\n",
    "# ステップ1で生成した生のログファイルを参照\n",
    "INPUT_CSV_PATH = os.path.join(BASE_DIR, 'data', 'presence_log.csv')\n",
    "# ステップD2の出力ファイル\n",
    "OUTPUT_CSV_PATH = os.path.join(BASE_DIR, 'data', 'eha_dataset_retirement.csv')\n",
    "LOG_FILE = os.path.join(BASE_DIR, 'log', 'eha_preparation_D.log')\n",
    "\n",
    "logging.basicConfig(filename=LOG_FILE, level=logging.INFO, filemode='w',\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "print(f\"Log will be saved to: {LOG_FILE}\")\n",
    "print(f\"Output CSV will be saved to: {OUTPUT_CSV_PATH}\")\n",
    "\n",
    "# --- 時代区分の定義 (仮説D用) ---\n",
    "# (EHA-Cと同じ定義を使用)\n",
    "PERIOD_PRE_WARRIOR = (1850, 1859)\n",
    "PERIOD_WARRIOR_ERA = (1860, 1869)\n",
    "PERIOD_PURGE_ERA = (1870, 1879)\n",
    "\n",
    "# --- ヘルパー関数 (EHA-Cと同様) ---\n",
    "def parse_eha_date(date_str):\n",
    "    if pd.isna(date_str):\n",
    "        return pd.NaT\n",
    "    date_str = str(date_str).strip()\n",
    "    try:\n",
    "        if re.match(r'^\\d{4}$', date_str): # YYYY\n",
    "            return pd.to_datetime(f\"{date_str}-01-01\")\n",
    "        if re.match(r'^\\d{4}-\\d{2}$', date_str): # YYYY-MM\n",
    "            return pd.to_datetime(f\"{date_str}-01\")\n",
    "        if re.match(r'^\\d{4}-\\d{2}-\\d{2}', date_str): # YYYY-MM-DD...\n",
    "            return pd.to_datetime(date_str.split('T')[0])\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Could not parse date: {date_str}. Error: {e}\")\n",
    "        return pd.NaT\n",
    "    return pd.NaT\n",
    "\n",
    "def calculate_duration_years(start_date, end_date):\n",
    "    if pd.isna(start_date) or pd.isna(end_date):\n",
    "        return np.nan\n",
    "    try:\n",
    "        duration_days = (end_date - start_date).days\n",
    "        if duration_days <= 0:\n",
    "             return np.nan\n",
    "        return duration_days / 365.25\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def assign_period_flag_D(start_year):\n",
    "    \"\"\"仮説D用の時代区分フラグを割り当てる\"\"\"\n",
    "    if pd.isna(start_year):\n",
    "        return \"Other_Pre_1850\" # ベースライン\n",
    "    start_year = int(start_year)\n",
    "    if PERIOD_PRE_WARRIOR[0] <= start_year <= PERIOD_PRE_WARRIOR[1]:\n",
    "        return \"PreWarrior_1850s\"\n",
    "    if PERIOD_WARRIOR_ERA[0] <= start_year <= PERIOD_WARRIOR_ERA[1]:\n",
    "        return \"WarriorEra_1860s\"\n",
    "    if PERIOD_PURGE_ERA[0] <= start_year <= PERIOD_PURGE_ERA[1]:\n",
    "        return \"PurgeEra_1870s\"\n",
    "    if start_year < 1850:\n",
    "        return \"Other_Pre_1850\" # ベースライン\n",
    "    return \"Other_Post_1880\"\n",
    "\n",
    "def assign_category_group_D(category_code):\n",
    "    \"\"\"艦種グループを割り当てる (B/Cと同様)\"\"\"\n",
    "    if pd.isna(category_code):\n",
    "        return \"Unknown\"\n",
    "    code = str(category_code).split('_')[0]\n",
    "    if code.startswith('SC') or code.startswith('A') or code.startswith('P'):\n",
    "         return \"NewTech_SteamOrArmoured\" # ベースライン\n",
    "    if code in ['SL1', 'SL2', 'SL3', 'SL4']:\n",
    "        return \"Sailing_LineOfBattle\"\n",
    "    if code in ['SF5', 'SC6']:\n",
    "        return \"Sailing_FrigateCorvette\"\n",
    "    if code in ['SSL', 'SBS', 'SCS', 'SM']:\n",
    "        return \"Sailing_Other\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "# --- メイン処理 ---\n",
    "def prepare_data_D():\n",
    "    print(\"--- Starting EHA Data Preparation (Hypothesis D: Retirement/Competing Risks) ---\")\n",
    "    logging.info(\"--- Starting EHA Data Preparation (Hypothesis D) ---\")\n",
    "\n",
    "    # 1. CSVファイルを読み込む\n",
    "    try:\n",
    "        print(f\"Loading presence log from: {INPUT_CSV_PATH}\")\n",
    "        df = pd.read_csv(INPUT_CSV_PATH)\n",
    "        logging.info(f\"Loaded {len(df)} records from {INPUT_CSV_PATH}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERROR: File not found. {e}\")\n",
    "        logging.error(f\"File not found: {e}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR reading CSV files: {e}\")\n",
    "        logging.error(f\"Error reading CSV files: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. 艦船ごとにソートし、次の状態（next_status）カラムを作成\n",
    "    print(\"Sorting data and identifying next status...\")\n",
    "    logging.info(\"Sorting data and identifying next status...\")\n",
    "    df['start_date_dt'] = df['start_date'].apply(parse_eha_date)\n",
    "    df_sorted = df.dropna(subset=['start_date_dt']).sort_values(by=['ship', 'start_date_dt'])\n",
    "    df_sorted['next_status'] = df_sorted.groupby('ship')['status'].shift(-1)\n",
    "\n",
    "    # 3. 分析対象のフィルタリング (現役の期間)\n",
    "    # ★★★ 分析対象を変更 ★★★\n",
    "    analysis_statuses = ['InCommission']\n",
    "    df_analysis = df_sorted[df_sorted['status'].isin(analysis_statuses)].copy()\n",
    "\n",
    "    if df_analysis.empty:\n",
    "        print(\"ERROR: No records found with status 'InCommission'.\")\n",
    "        logging.error(\"No records found with status 'InCommission'.\")\n",
    "        return\n",
    "    print(f\"Filtered {len(df_analysis)} 'InCommission' records.\")\n",
    "    logging.info(f\"Filtered {len(df_analysis)} 'InCommission' records.\")\n",
    "\n",
    "    # 4. 日付データのパース\n",
    "    print(\"Parsing (remaining) dates...\")\n",
    "    logging.info(\"Parsing (remaining) dates...\")\n",
    "    df_analysis['birth_date_dt'] = df_analysis['birth_date'].apply(parse_eha_date)\n",
    "    df_analysis['end_date_dt'] = df_analysis['end_date'].apply(parse_eha_date)\n",
    "\n",
    "    # 必要な日付データが欠損している行を除外\n",
    "    df_cleaned = df_analysis.dropna(subset=['birth_date_dt', 'start_date_dt', 'end_date_dt']).copy()\n",
    "    dropped_count = len(df_analysis) - len(df_cleaned)\n",
    "    if dropped_count > 0:\n",
    "        print(f\"Dropped {dropped_count} rows due to missing essential date data.\")\n",
    "        logging.warning(f\"Dropped {dropped_count} rows due to missing essential date data.\")\n",
    "\n",
    "    if df_cleaned.empty:\n",
    "        print(\"ERROR: No valid records remaining after date cleaning.\")\n",
    "        logging.error(\"No valid records remaining after date cleaning.\")\n",
    "        return\n",
    "\n",
    "    # 5. EHA変数の作成\n",
    "    print(\"Creating EHA variables (duration, events, age_at_start, period_flag, category_group)...\")\n",
    "    logging.info(\"Creating EHA variables...\")\n",
    "\n",
    "    # duration: 現役だった期間（年）\n",
    "    df_cleaned['duration'] = df_cleaned.apply(\n",
    "        lambda row: calculate_duration_years(row['start_date_dt'], row['end_date_dt']),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # ★★★ 競合リスク（Competing Risks）のイベント列を作成 ★★★\n",
    "\n",
    "    # イベントA: 退役 (→ InOrdinary)\n",
    "    df_cleaned['event_A_Retired'] = df_cleaned['next_status'].apply(\n",
    "        lambda x: 1 if x == 'InOrdinary' else 0\n",
    "    )\n",
    "\n",
    "    # イベントB: 転用 (→ InNonCombatDuty)\n",
    "    df_cleaned['event_B_Converted'] = df_cleaned['next_status'].apply(\n",
    "        lambda x: 1 if x == 'InNonCombatDuty' else 0\n",
    "    )\n",
    "\n",
    "    # イベントC: 処分 (→ キャリア終了, next_statusがNaN)\n",
    "    df_cleaned['event_C_Disposed'] = df_cleaned['next_status'].apply(\n",
    "        lambda x: 1 if pd.isna(x) else 0\n",
    "    )\n",
    "\n",
    "    # (参考) 総合イベント列 (いずれかが発生した場合 = 1)\n",
    "    df_cleaned['event_Any'] = (df_cleaned['event_A_Retired'] |\n",
    "                             df_cleaned['event_B_Converted'] |\n",
    "                             df_cleaned['event_C_Disposed'])\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "\n",
    "    # age_at_start: 現役が始まった時点の艦齢（年）\n",
    "    df_cleaned['age_at_start'] = df_cleaned.apply(\n",
    "        lambda row: calculate_duration_years(row['birth_date_dt'], row['start_date_dt']),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # period_flag: 時代区分\n",
    "    df_cleaned['start_year'] = df_cleaned['start_date_dt'].dt.year\n",
    "    df_cleaned['period_flag'] = df_cleaned['start_year'].apply(assign_period_flag_D)\n",
    "\n",
    "    # category_group: 艦種グループ\n",
    "    df_cleaned['category_group'] = df_cleaned['category_code'].apply(assign_category_group_D)\n",
    "\n",
    "    # 6. 最終クリーニング\n",
    "    df_final = df_cleaned.dropna(subset=['duration', 'age_at_start', 'category_group', 'period_flag'])\n",
    "    df_final = df_final[df_final['duration'] > 0]\n",
    "    df_final = df_final[df_final['category_group'] != 'Unknown']\n",
    "\n",
    "    # 少なくとも1つのイベントが発生している（打ち切りでない）行が対象\n",
    "    df_final = df_final[df_final['event_Any'] == 1]\n",
    "\n",
    "    print(f\"Final dataset contains {len(df_final)} observations for EHA (Hypothesis D).\")\n",
    "    logging.info(f\"Final dataset contains {len(df_final)} observations for EHA (Hypothesis D).\")\n",
    "\n",
    "    # 7. 最終的なCSVに保存\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(OUTPUT_CSV_PATH), exist_ok=True)\n",
    "        df_final.to_csv(OUTPUT_CSV_PATH, index=False, encoding='utf-8')\n",
    "\n",
    "        print(f\"Successfully saved EHA dataset for Retirement/Competing Risks to {OUTPUT_CSV_PATH}\")\n",
    "        logging.info(f\"Successfully saved EHA dataset for Retirement/Competing Risks to {OUTPUT_CSV_PATH}\")\n",
    "    except Exception as e:\n",
    "        msg = f\"Error saving CSV file to {OUTPUT_CSV_PATH}: {e}\"\n",
    "        print(msg)\n",
    "        logging.error(msg)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # データ整形処理を実行\n",
    "    prepare_data_D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7185,
     "status": "ok",
     "timestamp": 1762958210608,
     "user": {
      "displayName": "naoki cocaze",
      "userId": "16247746366338060822"
     },
     "user_tz": -540
    },
    "id": "f-Ed0WItzQ-g",
    "outputId": "4670019e-dbc2-4a89-d5e8-991fbcdbcae8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from lifelines import CoxPHFitter\n",
    "import logging\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# --- 設定 ---\n",
    "BASE_DIR = 'https://github.com/naoki-kokaze/British_Warship_Career/tree/main/'\n",
    "INPUT_CSV_PATH = os.path.join(BASE_DIR, 'data', 'eha_dataset_retirement.csv')\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, 'visualizations')\n",
    "LOG_FILE   = os.path.join(BASE_DIR, 'log', 'eha_analysis_D.log')\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(filename=LOG_FILE, level=logging.INFO, filemode='w',\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "print(f\"Log will be saved to: {LOG_FILE}\")\n",
    "print(f\"Visualization outputs will be saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "def stars(p):\n",
    "    \"\"\"p値に応じたスター表記\"\"\"\n",
    "    return \"***\" if p < 0.001 else \"**\" if p < 0.01 else \"*\" if p < 0.05 else \"\"\n",
    "\n",
    "def run_competing_risk_model(df_eha, event_column_name, analysis_name_suffix):\n",
    "    \"\"\"指定された event 列で CoxPH を実行し、HR 図に p値および有意・周辺有意のハイライトを付けて保存\"\"\"\n",
    "\n",
    "    print(f\"\\n--- Running CoxPH Model for Event: {event_column_name} ({analysis_name_suffix}) ---\")\n",
    "    logging.info(f\"--- Running CoxPH Model for Event: {event_column_name} ({analysis_name_suffix}) ---\")\n",
    "\n",
    "    # ダミー化\n",
    "    try:\n",
    "        df_cox = pd.get_dummies(\n",
    "            df_eha,\n",
    "            columns=['period_flag', 'category_group'],\n",
    "            prefix={'period_flag': 'period', 'category_group': 'cat'},\n",
    "            drop_first=True, dtype=int\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR creating dummy variables: {e}\")\n",
    "        logging.error(f\"ERROR creating dummy variables: {e}\")\n",
    "        return\n",
    "\n",
    "    period_dummies   = [c for c in df_cox.columns if c.startswith('period_')]\n",
    "    category_dummies = [c for c in df_cox.columns if c.startswith('cat_')]\n",
    "    covariates = ['age_at_start'] + period_dummies + category_dummies\n",
    "\n",
    "    # event列を共通名に変更\n",
    "    need_cols = ['duration', event_column_name] + covariates\n",
    "    missing = [c for c in need_cols if c not in df_cox.columns]\n",
    "    if missing:\n",
    "        print(f\"ERROR: Missing columns for CoxPH: {missing}\")\n",
    "        logging.error(f\"Missing columns for CoxPH: {missing}\")\n",
    "        return\n",
    "\n",
    "    df_cox_final = df_cox[need_cols].rename(columns={event_column_name: 'event'}).copy()\n",
    "    df_cox_final = df_cox_final.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "    if df_cox_final.empty or not covariates:\n",
    "        print(\"ERROR: No data or no covariates left for CoxPH.\")\n",
    "        logging.error(\"No data or no covariates left for CoxPH.\")\n",
    "        return\n",
    "\n",
    "    if df_cox_final['event'].sum() == 0:\n",
    "        print(f\"Warning: No events=1 for {event_column_name}. Skipping.\")\n",
    "        logging.warning(f\"No events=1 for {event_column_name}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Running CoxPH model on {len(df_cox_final)} observations with {len(covariates)} covariates.\")\n",
    "    logging.info(f\"Running CoxPH model on {len(df_cox_final)} observations with {len(covariates)} covariates.\")\n",
    "\n",
    "    cph = CoxPHFitter(penalizer=1e-5, l1_ratio=0.0)\n",
    "    try:\n",
    "        cph.fit(df_cox_final, duration_col='duration', event_col='event')\n",
    "        summary = cph.summary\n",
    "\n",
    "        # --- 結果保存 ---\n",
    "        summary_path = os.path.join(OUTPUT_DIR, f'coxph_summary_retirement_{analysis_name_suffix}.csv')\n",
    "        summary.to_csv(summary_path, encoding='utf-8-sig')\n",
    "        print(f\"Model summary saved to {summary_path}\")\n",
    "\n",
    "        # --- 図描画 ---\n",
    "        plt.figure(figsize=(10, max(12, len(covariates) * 0.5)))\n",
    "        ax = cph.plot(hazard_ratios=True)\n",
    "        ax.set_xscale('log')\n",
    "        ax.axvline(1, ls='--', color='gray', alpha=0.8)\n",
    "        ax.set_xticks([0.25, 0.5, 1, 2, 4])\n",
    "        ax.get_xaxis().set_major_formatter(mticker.ScalarFormatter())\n",
    "\n",
    "        # テキスト配置位置\n",
    "        x_min, x_max = ax.get_xlim()\n",
    "        x_text = x_max * 1.05\n",
    "\n",
    "        # y座標対応\n",
    "        yticks = ax.get_yticks()\n",
    "        ylabels = [t.get_text() for t in ax.get_yticklabels()]\n",
    "        label2y = {lab: y for lab, y in zip(ylabels, yticks)}\n",
    "\n",
    "        # --- ハイライト設定 ---\n",
    "        color_sig = '#FFF176'     # 濃い黄色（p<0.05）\n",
    "        color_marg = '#FFF9C4'    # 薄い黄色（0.05<=p<0.07）\n",
    "        alpha_band = 0.55\n",
    "\n",
    "        for var, row in summary.iterrows():\n",
    "            if var in label2y:\n",
    "                y = label2y[var]\n",
    "                pval = float(row.get('p', 1.0))\n",
    "\n",
    "                # 有意または周辺有意なら帯を描画\n",
    "                if pval < 0.05:\n",
    "                    ax.axhspan(y - 0.35, y + 0.35, color=color_sig, alpha=alpha_band, zorder=0)\n",
    "                elif 0.05 <= pval < 0.07:\n",
    "                    ax.axhspan(y - 0.35, y + 0.35, color=color_marg, alpha=alpha_band, zorder=0)\n",
    "\n",
    "                # p値テキスト\n",
    "                ax.text(x_text, y, f\"p = {pval:.3f}{stars(pval)}\", va='center', fontsize=10)\n",
    "\n",
    "        # テキスト分の右余白\n",
    "        ax.set_xlim(x_min, x_text * 1.12)\n",
    "\n",
    "        # 凡例追加\n",
    "        patch_sig = mpatches.Patch(facecolor=color_sig, alpha=alpha_band, label='p < 0.05')\n",
    "        patch_marg = mpatches.Patch(facecolor=color_marg, alpha=alpha_band, label='0.05 ≤ p < 0.07')\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        ax.legend(handles + [patch_sig, patch_marg],\n",
    "                  labels + [patch_sig.get_label(), patch_marg.get_label()],\n",
    "                  loc='upper right', title='Significance')\n",
    "\n",
    "        plt.title(f'CoxPH Model: Hazard Ratios (exp(coef)) with Significance Bands\\n({analysis_name_suffix})', fontsize=14)\n",
    "        plt.xlabel('Hazard Ratio (exp(coef))', fontsize=12)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "        out_svg = os.path.join(OUTPUT_DIR, f'coxph_plot_retirement_{analysis_name_suffix}_with_p_highlighted.svg')\n",
    "        plt.savefig(out_svg, format='svg', bbox_inches='tight')\n",
    "\n",
    "        print(f\"Model coefficients plot saved to {out_svg}\")\n",
    "        logging.info(f\"Model coefficients plot saved to {out_svg}\")\n",
    "        plt.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: CoxPH model fitting failed for {analysis_name_suffix}: {e}\")\n",
    "        logging.exception(f\"CoxPH model fitting failed for {analysis_name_suffix}: {e}\")\n",
    "\n",
    "def run_eha_analysis_D():\n",
    "    print(\"--- Starting EHA Analysis (Hypothesis D: Retirement/Competing Risks) ---\")\n",
    "    logging.info(\"--- Starting EHA Analysis (Hypothesis D) ---\")\n",
    "\n",
    "    try:\n",
    "        df_eha_base = pd.read_csv(INPUT_CSV_PATH)\n",
    "        logging.info(f\"Loaded {len(df_eha_base)} records from {INPUT_CSV_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading file: {e}\")\n",
    "        return\n",
    "\n",
    "    # 各イベントを順に分析\n",
    "    for ev, name in [\n",
    "        ('event_A_Retired', 'A_Retired_to_Ordinary'),\n",
    "        ('event_B_Converted', 'B_Converted_to_NonCombat'),\n",
    "        ('event_C_Disposed', 'C_Disposed_from_Commission')\n",
    "    ]:\n",
    "        if ev in df_eha_base.columns:\n",
    "            run_competing_risk_model(df_eha_base, ev, name)\n",
    "        else:\n",
    "            print(f\"Column {ev} not found. Skipping.\")\n",
    "\n",
    "    print(\"\\n--- All EHA-D Analyses Finished ---\")\n",
    "    logging.info(\"--- All EHA-D Analyses Finished ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_eha_analysis_D()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPUiMfVwVreQ7P44gyhplWr",
   "collapsed_sections": [
    "MYYFuBBUO4qJ"
   ],
   "mount_file_id": "1XBBohN-3T5XyCclDXRsXkA_FydbaOPhs",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
