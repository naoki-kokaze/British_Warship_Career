{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8190,
     "status": "ok",
     "timestamp": 1762753587537,
     "user": {
      "displayName": "naoki cocaze",
      "userId": "16247746366338060822"
     },
     "user_tz": -540
    },
    "id": "dzmuVWG3eF5N",
    "outputId": "64a57766-73f7-4d85-e260-6fb71cd60166"
   },
   "outputs": [],
   "source": [
    "!pip install rdflib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4akTKwpAg-F6"
   },
   "source": [
    "## 検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 163700,
     "status": "ok",
     "timestamp": 1762754044430,
     "user": {
      "displayName": "naoki cocaze",
      "userId": "16247746366338060822"
     },
     "user_tz": -540
    },
    "id": "odoV-127QsHZ",
    "outputId": "a1443104-7c29-496f-c4aa-fb21cacbbd3c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from rdflib import Graph\n",
    "from rdflib.exceptions import ParserError\n",
    "import logging\n",
    "\n",
    "# --- 設定 ---\n",
    "BASE_DIR = 'https://github.com/naoki-kokaze/British_Warship_Career/tree/main/'\n",
    "TURTLE_DIR = os.path.join(BASE_DIR, 'api_out')  # 生成されたTTLファイルがあるディレクトリ\n",
    "ONTOLOGY_PATH = os.path.join(BASE_DIR, 'scheme', 'warship_career_ontology.ttl') # オントロジーファイルのパス\n",
    "LOG_FILE = os.path.join(BASE_DIR, 'log', 'validation.log') # 検証ログファイル\n",
    "STRATA_DIR = os.path.join(BASE_DIR, 'strata')  # 原文Excelファイルがあるディレクトリ\n",
    "excel_cache = {}  # ★★★ Excel読み込みを高速化するキャッシュ\n",
    "\n",
    "# --- ロギング設定 ---\n",
    "\n",
    "# 1. ログファイルの親ディレクトリを取得\n",
    "log_dir = os.path.dirname(LOG_FILE)\n",
    "\n",
    "# 2. 親ディレクトリが存在するかチェックし、なければ作成する\n",
    "if log_dir and not os.path.exists(log_dir):\n",
    "    try:\n",
    "        os.makedirs(log_dir)\n",
    "        print(f\"Log directory not found. Created: {log_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL: Could not create log directory {log_dir}: {e}\")\n",
    "        print(\"Validation cannot proceed without logging.\")\n",
    "        exit() # ディレクトリが作れなければ終了する\n",
    "\n",
    "# filemode='a' (追記モード) に変更\n",
    "logging.basicConfig(filename=LOG_FILE, level=logging.INFO, filemode='a',\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "print(f\"Validation log will be appended to: {LOG_FILE}\")\n",
    "logging.info(f\"--- Validation Process Started ---\") # ログに開始を記録\n",
    "\n",
    "# --- 検証関数 ---\n",
    "\n",
    "def validate_syntax(filepath):\n",
    "    \"\"\"(フェーズ1) Turtleファイルの構文を検証する\"\"\"\n",
    "    g = Graph()\n",
    "    try:\n",
    "        g.parse(filepath, format=\"turtle\")\n",
    "        logging.info(f\"[Syntax OK] {os.path.basename(filepath)}\")\n",
    "        return True\n",
    "    except ParserError as e:\n",
    "        logging.error(f\"[Syntax Error] {os.path.basename(filepath)}: {e}\")\n",
    "        print(f\"[Syntax Error] {os.path.basename(filepath)}: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[File Error] {os.path.basename(filepath)}: {e}\")\n",
    "        print(f\"[File Error] {os.path.basename(filepath)}: {e}\")\n",
    "        return False\n",
    "\n",
    "def run_sparql_validation(graph, filename):\n",
    "    \"\"\"SPARQLクエリを使ってカスタムルールを検証する\"\"\"\n",
    "    all_rules_passed = True\n",
    "    validation_queries = {\n",
    "        \"Multiple Statuses per Presence\": \"\"\"\n",
    "            PREFIX crm: <http://www.cidoc-crm.org/cidoc-crm/>\n",
    "            PREFIX : <http://www.example.com/ontology/warship#>\n",
    "            SELECT ?presence (COUNT(DISTINCT ?status) AS ?count)\n",
    "            WHERE {\n",
    "              ?presence a crm:E93_Presence ;\n",
    "                        :has_operational_status ?status .\n",
    "            } GROUP BY ?presence HAVING (COUNT(DISTINCT ?status) > 1)\n",
    "            \"\"\",\n",
    "        \"Role and Type Conflict\": \"\"\"\n",
    "            PREFIX crm: <http://www.cidoc-crm.org/cidoc-crm/>\n",
    "            PREFIX : <http://www.example.com/ontology/warship#>\n",
    "            SELECT ?presence\n",
    "            WHERE {\n",
    "              ?presence a crm:E93_Presence ;\n",
    "                        :has_ship_role ?role ;\n",
    "                        :has_ship_type ?type .\n",
    "            }\n",
    "            \"\"\",\n",
    "        \"Type Missing (No Role)\": \"\"\"\n",
    "            PREFIX crm: <http://www.cidoc-crm.org/cidoc-crm/>\n",
    "            PREFIX : <http://www.example.com/ontology/warship#>\n",
    "            PREFIX myData: <http://www.example.com/myData/>\n",
    "            SELECT ?presence\n",
    "            WHERE {\n",
    "              ?presence a crm:E93_Presence ;\n",
    "                        :has_operational_status ?status .\n",
    "              FILTER (?status IN (myData:InCommission, myData:InOrdinary, myData:UnderRepair))\n",
    "              FILTER NOT EXISTS { ?presence :has_ship_role ?role }\n",
    "              FILTER NOT EXISTS { ?presence :has_ship_type ?type }\n",
    "            }\n",
    "            \"\"\",\n",
    "         \"Type Unexpected (With Role)\": \"\"\"\n",
    "            PREFIX crm: <http://www.cidoc-crm.org/cidoc-crm/>\n",
    "            PREFIX : <http://www.example.com/ontology/warship#>\n",
    "            PREFIX myData: <http://www.example.com/myData/>\n",
    "            SELECT ?presence\n",
    "            WHERE {\n",
    "              { ?presence :has_operational_status myData:InNonCombatDuty . }\n",
    "              UNION\n",
    "              { ?presence :has_ship_role ?anyRole . }\n",
    "\n",
    "              ?presence :has_ship_type ?type .\n",
    "            }\n",
    "            \"\"\"\n",
    "    }\n",
    "\n",
    "    for rule_name, query in validation_queries.items():\n",
    "        try:\n",
    "            results = graph.query(query)\n",
    "            if len(results) > 0:\n",
    "                all_rules_passed = False\n",
    "                msg = f\"[Custom Rule Error] {filename}: Rule '{rule_name}' violated for:\"\n",
    "                logging.error(msg)\n",
    "                print(msg)\n",
    "                for row in results:\n",
    "                    # Try to get the presence URI, handle potential attribute errors if query differs\n",
    "                    try:\n",
    "                       p_uri = row.presence\n",
    "                       logging.error(f\"  - {p_uri}\")\n",
    "                       print(f\"  - {p_uri}\")\n",
    "                    except AttributeError:\n",
    "                       logging.error(f\"  - Query result row: {row}\") # Log the full row if 'presence' isn't the variable\n",
    "                       print(f\"  - Query result row: {row}\")\n",
    "        except Exception as e:\n",
    "             all_rules_passed = False\n",
    "             msg = f\"[SPARQL Error] {filename}: Query for rule '{rule_name}' failed: {e}\"\n",
    "             logging.error(msg)\n",
    "             print(msg)\n",
    "\n",
    "    if all_rules_passed:\n",
    "        logging.info(f\"[Rules OK] {filename}\")\n",
    "    return all_rules_passed\n",
    "\n",
    "def check_missing_costs(graph, filename):\n",
    "    \"\"\"SPARQLクエリを使って :incurred_cost が欠落している可能性のあるイベントをチェック\"\"\"\n",
    "    query_missing_cost = \"\"\"\n",
    "    PREFIX : <http://www.example.com/ontology/warship#>\n",
    "    PREFIX crm: <http://www.cidoc-crm.org/cidoc-crm/>\n",
    "    PREFIX sealit: <http://www.sealitproject.eu/ontology/>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "\n",
    "    SELECT ?event ?eventType ?eventLabel\n",
    "    WHERE {\n",
    "      VALUES ?baseEventType {\n",
    "        crm:E12_Production\n",
    "        sealit:ShipRepair # Includes subclasses via rdfs:subClassOf*\n",
    "        :FittingForSea\n",
    "        :ShipConversion  # Includes subclasses via rdfs:subClassOf*\n",
    "      }\n",
    "      ?eventType rdfs:subClassOf* ?baseEventType .\n",
    "      ?event a ?eventType .\n",
    "      OPTIONAL { ?event rdfs:label ?eventLabel . }\n",
    "      FILTER NOT EXISTS {\n",
    "        ?event :incurred_cost ?cost .\n",
    "      }\n",
    "    }\n",
    "    ORDER BY ?event\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = graph.query(query_missing_cost)\n",
    "        if len(results) > 0:\n",
    "            msg = f\"[Cost Check Warning] {filename}: Found potential missing :incurred_cost for the following events (Manual verification needed):\"\n",
    "            logging.warning(msg)\n",
    "            print(msg)\n",
    "            for row in results:\n",
    "                event_type_short = str(row.eventType).split('/')[-1].split('#')[-1]\n",
    "                log_line = f\"  - Event: {row.event}, Type: {event_type_short}, Label: {row.eventLabel}\"\n",
    "                logging.warning(log_line)\n",
    "                print(log_line)\n",
    "            return False # Indicate potential issue found\n",
    "        else:\n",
    "            logging.info(f\"[Cost Check OK] {filename}: No obvious missing :incurred_cost found.\")\n",
    "            return True # Indicate no obvious issue found\n",
    "    except Exception as e:\n",
    "        msg = f\"[SPARQL Error] {filename}: SPARQL query for missing costs failed: {e}\"\n",
    "        logging.error(msg)\n",
    "        print(msg)\n",
    "        return False # Indicate error during check\n",
    "\n",
    "def load_original_text(turtle_filepath):\n",
    "    \"\"\"\n",
    "    (フェーズ3の準備)\n",
    "    .ttl ファイルパスから対応する .xlsx ファイルを特定し、\n",
    "    艦船IDに基づいて原文テキストを読み込む。\n",
    "    \"\"\"\n",
    "    global excel_cache # Excelファイルのキャッシュをグローバルに使用\n",
    "\n",
    "    try:\n",
    "        filename = os.path.basename(turtle_filepath) # 例: \"SL1_0016.ttl\"\n",
    "\n",
    "        # 1. 艦種コード (Strata) をファイル名から抽出 (例: \"SL1\")\n",
    "        match = re.match(r'([^_]+)_', filename)\n",
    "        if not match:\n",
    "            logging.warning(f\"  [Fact Check SKIPPED] Could not extract strata code from filename: {filename}\")\n",
    "            return None\n",
    "        strata_code = match.group(1) # \"SL1\"\n",
    "\n",
    "        # 2. 艦船ID (Ship ID) を構築 (例: \"myData:SL1_0016\")\n",
    "        ship_id_base = os.path.splitext(filename)[0] # \"SL1_0016\"\n",
    "        ship_id_uri = f\"myData:{ship_id_base}\" # \"myData:SL1_0016\"\n",
    "\n",
    "        # 3. Excelファイルのパスを構築\n",
    "        excel_filename = f\"{strata_code}.xlsx\" # \"SL1.xlsx\"\n",
    "        excel_filepath = os.path.join(STRATA_DIR, excel_filename)\n",
    "\n",
    "        # 4. Excelファイルを（キャッシュを利用しつつ）読み込む\n",
    "        if excel_filepath not in excel_cache:\n",
    "            if not os.path.exists(excel_filepath):\n",
    "                logging.warning(f\"  [Fact Check SKIPPED] Strata Excel file not found: {excel_filepath}\")\n",
    "                return None\n",
    "\n",
    "            # Excelの読み込みは時間がかかるため、初回のみ実行\n",
    "            print(f\"  Loading strata file: {excel_filename}...\")\n",
    "            excel_cache[excel_filepath] = pd.read_excel(excel_filepath)\n",
    "            logging.info(f\"  Loaded and cached strata file: {excel_filepath}\")\n",
    "\n",
    "        df_strata = excel_cache[excel_filepath]\n",
    "\n",
    "        # 5. DataFrameから該当IDの行を検索し、\"text\"カラムの値を返す\n",
    "        #    'ID' カラムが 'myData:SL1_0016' と完全に一致する行を探す\n",
    "        row = df_strata[df_strata['ID'] == ship_id_uri]\n",
    "\n",
    "        if row.empty:\n",
    "            logging.warning(f\"  [Fact Check SKIPPED] ID '{ship_id_uri}' not found in 'ID' column of {excel_filename}\")\n",
    "            return None\n",
    "\n",
    "        original_text = row.iloc[0]['text']\n",
    "\n",
    "        if pd.isna(original_text):\n",
    "            logging.warning(f\"  [Fact Check SKIPPED] 'text' column is empty for ID '{ship_id_uri}' in {excel_filename}\")\n",
    "            return None\n",
    "\n",
    "        return str(original_text)\n",
    "\n",
    "    except ImportError:\n",
    "        logging.error(\"  [Fact Check FATAL] `pandas` or `openpyxl` is not installed. Fact checking requires them. Run: pip install pandas openpyxl\")\n",
    "        print(\"  [Fact Check FATAL] `pandas` or `openpyxl` is not installed. Fact checking requires them. Run: pip install pandas openpyxl\")\n",
    "        return \"STOP\" # 特殊なエラーコードを返し、メインループを止める\n",
    "    except Exception as e:\n",
    "        logging.error(f\"  [Fact Check ERROR] Could not read original text for {filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "def check_fact_consistency(merged_graph, filename, original_text):\n",
    "    \"\"\"\n",
    "    (フェーズ3) 原文テキストとRDFの「事実」（費用、年）を比較し、\n",
    "    ハルシネーションや見落としの可能性を警告する。\n",
    "    \"\"\"\n",
    "    print(\"Checking fact consistency (Text vs RDF)...\")\n",
    "\n",
    "    # 1. 原文テキストから「事実」を抽出 (Regex)\n",
    "    #    (£1,234 や 4,030 のような数値。£記号はあってもなくても良い)\n",
    "    text_costs = set(re.findall(r'£?([\\d,]+(?:\\.\\d{1,2})?)\\b', original_text))\n",
    "    # コンマを削除して正規化\n",
    "    normalized_text_costs = {c.replace(',', '') for c in text_costs}\n",
    "\n",
    "    # 年 (4桁の年号)\n",
    "    text_years = set(re.findall(r'\\b(1[7-9]\\d{2})\\b', original_text))\n",
    "\n",
    "    # 2. RDFから「事実」を抽出 (SPARQL)\n",
    "    query_rdf_facts = \"\"\"\n",
    "    SELECT (str(?costVal) as ?cost) (str(?dateVal) as ?date)\n",
    "    WHERE {\n",
    "      { ?event :incurred_cost [ crm:P90_has_value ?costVal ] . }\n",
    "      UNION\n",
    "      {\n",
    "        ?event crm:P4_has_time-span ?ts .\n",
    "        { ?ts crm:P79_beginning_is_qualified_by ?dateVal . }\n",
    "        UNION\n",
    "        { ?ts crm:P80_end_is_qualified_by ?dateVal . }\n",
    "        UNION\n",
    "        { ?ts crm:P82_at_some_time_within ?dateVal . }\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    "    rdf_costs = set()\n",
    "    rdf_years = set()\n",
    "\n",
    "    try:\n",
    "        results = merged_graph.query(query_rdf_facts)\n",
    "        for row in results:\n",
    "            if row.cost:\n",
    "                # '25352.0' -> '25352' のように正規化\n",
    "                rdf_costs.add(str(row.cost).split('.')[0])\n",
    "            if row.date:\n",
    "                # '1836-01-21' -> '1836' のように年号だけ抽出\n",
    "                year_match = re.search(r'\\b(1[7-9]\\d{2})\\b', str(row.date))\n",
    "                if year_match:\n",
    "                    rdf_years.add(year_match.group(1))\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"  [Fact Check ERROR] {filename}: SPARQL query for facts failed: {e}\")\n",
    "        return False # チェック失敗\n",
    "\n",
    "    # 3. 突合と警告\n",
    "    has_warnings = False\n",
    "\n",
    "    # 警告A: RDFにある費用がテキストにない (捏造)\n",
    "    # (テキスト側は \"£4,030\" と \"4,030\" の両方を許容する)\n",
    "    hallucinated_costs = {cost for cost in rdf_costs if cost not in normalized_text_costs}\n",
    "    if hallucinated_costs:\n",
    "        has_warnings = True\n",
    "        logging.warning(f\"  [Fact Mismatch WARNING] {filename}: Costs found in RDF but NOT in TEXT (Possible Hallucination): {hallucinated_costs}\")\n",
    "\n",
    "    # 警告B: テキストにある費用がRDFにない (見落とし)\n",
    "    # (RDF側は '4030'、テキスト側は '£4,030' や '4030' を想定)\n",
    "    missing_costs = {cost for cost in normalized_text_costs if cost not in rdf_costs}\n",
    "    if missing_costs:\n",
    "        has_warnings = True\n",
    "        logging.warning(f\"  [Fact MMismatch WARNING] {filename}: Costs found in TEXT but NOT in RDF (Possible Omission): {missing_costs}\")\n",
    "\n",
    "    # 警告C: テキストにある年がRDFにない (イベント見落とし)\n",
    "    missing_years = text_years - rdf_years\n",
    "    if missing_years:\n",
    "        has_warnings = True\n",
    "        logging.warning(f\"  [Fact Mismatch WARNING] {filename}: Years found in TEXT but NOT in RDF Events (Possible Event Omission): {missing_years}\")\n",
    "\n",
    "    if not has_warnings:\n",
    "        logging.info(f\"  [Fact Check OK] {filename}: Costs and Years seem consistent.\")\n",
    "        print(\"  [Fact Check OK] Costs and Years seem consistent.\")\n",
    "    else:\n",
    "        print(f\"  [Fact Mismatch WARNING] {filename}: Check logs for potential fact omissions or hallucinations.\")\n",
    "\n",
    "    return True # チェック自体は完了\n",
    "\n",
    "# ★★★ メイン実行ブロック ★★★\n",
    "# TURTLE_DIR (親) の配下にあるサブディレクトリをループ処理する\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(f\"--- Starting Validation Process ---\")\n",
    "    print(f\"Base Directory: {TURTLE_DIR}\")\n",
    "    print(f\"Strata Directory (for source text): {STRATA_DIR}\")\n",
    "    print(f\"Ontology File: {ONTOLOGY_PATH}\")\n",
    "    print(f\"Log File: {LOG_FILE}\")\n",
    "    logging.info(f\"Base Directory: {TURTLE_DIR}\")\n",
    "    logging.info(f\"Ontology File: {ONTOLOGY_PATH}\")\n",
    "\n",
    "    # オントロジーファイルを読み込む (ループの外で1回だけ)\n",
    "    ontology_graph = Graph()\n",
    "    try:\n",
    "         ontology_graph.parse(ONTOLOGY_PATH, format=\"turtle\")\n",
    "         logging.info(f\"Successfully loaded ontology: {ONTOLOGY_PATH}\")\n",
    "         print(f\"Successfully loaded ontology: {ONTOLOGY_PATH}\")\n",
    "    except Exception as e:\n",
    "         logging.error(f\"FATAL: Error loading ontology file {ONTOLOGY_PATH}: {e}\")\n",
    "         print(f\"FATAL: Error loading ontology file {ONTOLOGY_PATH}: {e}\")\n",
    "         print(\"Validation cannot proceed without the ontology.\")\n",
    "         logging.error(\"Validation cannot proceed without the ontology.\")\n",
    "         exit()\n",
    "\n",
    "    # --- サブディレクトリの検出 ---\n",
    "    if not os.path.isdir(TURTLE_DIR):\n",
    "        print(f\"ERROR: Base directory not found: {TURTLE_DIR}\")\n",
    "        logging.error(f\"ERROR: Base directory not found: {TURTLE_DIR}\")\n",
    "        exit()\n",
    "\n",
    "    try:\n",
    "        sub_dirs = [d for d in os.listdir(TURTLE_DIR)\n",
    "                    if os.path.isdir(os.path.join(TURTLE_DIR, d))]\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not list subdirectories in {TURTLE_DIR}: {e}\")\n",
    "        logging.error(f\"ERROR: Could not list subdirectories in {TURTLE_DIR}: {e}\")\n",
    "        exit()\n",
    "\n",
    "    if not sub_dirs:\n",
    "        print(f\"No subdirectories found in {TURTLE_DIR}. Nothing to validate.\")\n",
    "        logging.warning(f\"No subdirectories found in {TURTLE_DIR}. Nothing to validate.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"Found {len(sub_dirs)} subdirectories to process: {sub_dirs}\")\n",
    "    logging.info(f\"Found {len(sub_dirs)} subdirectories to process: {sub_dirs}\")\n",
    "\n",
    "    # --- 各サブディレクトリをループ処理 ---\n",
    "    for sub_dir_name in sub_dirs:\n",
    "        current_turtle_dir = os.path.join(TURTLE_DIR, sub_dir_name)\n",
    "\n",
    "        print(f\"\\n\\n=======================================================\")\n",
    "        print(f\"--- Processing Directory: {current_turtle_dir} ---\")\n",
    "        print(f\"=======================================================\")\n",
    "        logging.info(f\"--- Processing Directory: {current_turtle_dir} ---\")\n",
    "\n",
    "        # ディレクトリごとに統計をリセット\n",
    "        syntax_errors = 0\n",
    "        rule_violations = 0 # 「要確認」フラグの立ったファイル数\n",
    "        processed_files = 0\n",
    "        syntax_ok_files = []\n",
    "        all_files = []\n",
    "\n",
    "        # 1. 構文検証フェーズ\n",
    "        print(f\"\\n--- Phase 1: Syntax Validation (Directory: {sub_dir_name}) ---\")\n",
    "        try:\n",
    "            all_files = [f for f in os.listdir(current_turtle_dir) if f.endswith('.ttl')]\n",
    "        except Exception as e:\n",
    "             print(f\"ERROR: Could not read directory {current_turtle_dir}: {e}\")\n",
    "             logging.error(f\"ERROR: Could not read directory {current_turtle_dir}: {e}\")\n",
    "             continue # 次のディレクトリへ\n",
    "\n",
    "        if not all_files:\n",
    "            print(f\"No .ttl files found in {current_turtle_dir}\")\n",
    "            logging.warning(f\"No .ttl files found in {current_turtle_dir}\")\n",
    "            # このディレクトリのサマリーを出力して次へ\n",
    "        else:\n",
    "            for filename in all_files:\n",
    "                filepath = os.path.join(current_turtle_dir, filename)\n",
    "                if validate_syntax(filepath):\n",
    "                    syntax_ok_files.append(filename)\n",
    "                else:\n",
    "                    syntax_errors += 1\n",
    "\n",
    "        print(f\"Syntax Validation Complete for {sub_dir_name}. {len(syntax_ok_files)} files OK, {syntax_errors} files with errors.\")\n",
    "        logging.info(f\"Syntax Validation Complete for {sub_dir_name}. {len(syntax_ok_files)} files OK, {syntax_errors} files with errors.\")\n",
    "\n",
    "        # 2. & 3. ルール / コスト / 事実突合チェックフェーズ\n",
    "        print(f\"\\n--- Phase 2 & 3: Rules, Costs, and Fact Checks (Directory: {sub_dir_name}) ---\")\n",
    "        if not syntax_ok_files:\n",
    "            print(\"No files passed syntax validation. Skipping further checks for this directory.\")\n",
    "            logging.warning(\"No files passed syntax validation. Skipping further checks for this directory.\")\n",
    "        else:\n",
    "            for filename in syntax_ok_files:\n",
    "                processed_files += 1\n",
    "                filepath = os.path.join(current_turtle_dir, filename)\n",
    "                g = Graph()\n",
    "\n",
    "                try:\n",
    "                    # --- 原文テキストの読み込み (フェーズ3準備) ---\n",
    "                    original_text_content = load_original_text(filepath)\n",
    "                    if original_text_content == \"STOP\":\n",
    "                        print(\"Stopping validation due to missing libraries (pandas/openpyxl).\")\n",
    "                        logging.error(\"Stopping validation due to missing libraries (pandas/openpyxl).\")\n",
    "                        break # このサブディリループを中断\n",
    "\n",
    "                    # --- グラフのマージ ---\n",
    "                    g.parse(filepath, format=\"turtle\")\n",
    "                    merged_graph = g + ontology_graph # オントロジーと結合\n",
    "\n",
    "                    print(f\"\\n--- Validating file: {filename} ({processed_files}/{len(syntax_ok_files)}) ---\")\n",
    "\n",
    "                    # --- (フェーズ2a) カスタムルール検証 ---\n",
    "                    print(\"Checking custom rules...\")\n",
    "                    rules_ok = run_sparql_validation(merged_graph, filename)\n",
    "\n",
    "                    # --- (フェーズ2b) 費用欠落チェック ---\n",
    "                    print(\"Checking for potentially missing costs...\")\n",
    "                    costs_ok = check_missing_costs(merged_graph, filename)\n",
    "\n",
    "                    # --- (フェーズ3) 事実突合チェック ---\n",
    "                    facts_ok = False\n",
    "                    if original_text_content:\n",
    "                        facts_ok = check_fact_consistency(merged_graph, filename, original_text_content)\n",
    "                    else:\n",
    "                        print(\"  [Fact Check SKIPPED] No original text file found.\")\n",
    "                        facts_ok = True # テキストがない場合はスキップ（エラー扱いしない）\n",
    "\n",
    "                    # 判定\n",
    "                    if rules_ok and costs_ok and facts_ok:\n",
    "                         logging.info(f\"[Overall OK] {filename}\")\n",
    "                         print(f\"[Overall OK] {filename}\")\n",
    "                    else:\n",
    "                         rule_violations += 1\n",
    "                         logging.warning(f\"[Validation Issues] {filename} - Check logs for details.\")\n",
    "                         print(f\"[Validation Issues] {filename} - Check logs for details.\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    rule_violations += 1\n",
    "                    logging.error(f\"[Processing Error] Could not process {filename} for validation: {e}\")\n",
    "                    print(f\"[Processing Error] Could not process {filename} for validation: {e}\")\n",
    "\n",
    "            if original_text_content == \"STOP\":\n",
    "                break # ライブラリエラーの場合、サブディレクトリのループも抜ける\n",
    "\n",
    "        # --- ディレクトリごとの最終結果表示 ---\n",
    "        print(f\"\\n--- Validation Summary for {sub_dir_name} ---\")\n",
    "        print(f\"Total files found: {len(all_files)}\")\n",
    "        print(f\"Files with Syntax Errors: {syntax_errors}\")\n",
    "        print(f\"Files checked for Rules/Costs/Facts: {len(syntax_ok_files)}\")\n",
    "        print(f\"Files with Issues (Warnings or Errors): {rule_violations}\")\n",
    "        logging.info(f\"--- Validation Summary for {sub_dir_name} ---\")\n",
    "        logging.info(f\"Total files found: {len(all_files)}\")\n",
    "        logging.info(f\"Files with Syntax Errors: {syntax_errors}\")\n",
    "        logging.info(f\"Files checked for Rules/Costs/Facts: {len(syntax_ok_files)}\")\n",
    "        logging.info(f\"Files with Issues (Warnings or Errors): {rule_violations}\")\n",
    "\n",
    "    print(\"\\n--- All Directories Processed ---\")\n",
    "    print(f\"Log file: {LOG_FILE}\")\n",
    "    logging.info(\"--- All Directories Processed ---\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM6QEb7xS/fp/iNyU5Ttx5S",
   "mount_file_id": "1x9HXQiGxvJTSfCR8RSQE1H2Yf7k3pVf7",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
